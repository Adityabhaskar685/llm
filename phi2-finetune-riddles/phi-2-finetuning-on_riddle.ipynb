{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning Phi2 on custom riddle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# packages \n",
    "!pip install -U accelerate peft transformers einops datasets bitsandbytes --q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerate==0.26.1\n",
      "bitsandbytes==0.42.0\n",
      "datasets==2.16.1\n",
      "einops==0.7.0\n",
      "peft==0.8.2\n",
      "torch==2.1.2+cu121\n",
      "transformers==4.37.2\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | egrep \"accelerate|peft|transformers|einops|datasets|^torch=|bitsandbytes\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed \n",
    "from transformers import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Tokenizer\n",
    "\n",
    "### Loading Model in 4bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f247a70a0c741dea0af5e2bc550625a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# model\n",
    "model_path = 'microsoft/phi-2'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map = 'auto',\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit =True,\n",
    "        bnb_4bit_compute_dtype = torch.bfloat16,\n",
    "        bnb_4bit_quant_type = 'nf4'\n",
    "    ),\n",
    "    torch_dtype = torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiForCausalLM(\n",
       "  (model): PhiModel(\n",
       "    (embed_tokens): Embedding(51200, 2560)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x PhiDecoderLayer(\n",
       "        (self_attn): PhiAttention(\n",
       "          (q_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
       "          (dense): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
       "          (rotary_emb): PhiRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): PhiMLP(\n",
       "          (activation_fn): NewGELUActivation()\n",
       "          (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50295"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add ChatML tokens to tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_tokens(['<|im_start|>', \"<PAD>\"])\n",
    "tokenizer.pad_token = '<PAD>'\n",
    "tokenizer.add_special_tokens(dict(eos_token = '<|im_end|>'))\n",
    "model.config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: there is no need to rezise the token embeddings, phi-2 already has embeddings sized for additional tokens. The model's vocab. size is 51200, this means you can add ~700 tokens to the tokenizer without having to resize the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(51200, 2560)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.embed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare LoRA adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing = True)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = 32,\n",
    "    lora_alpha = 32,\n",
    "    target_modules = ['q_proj','k_proj','v_proj','dense'],\n",
    "    modules_to_save = ['lm_head','embed_tokens'],\n",
    "    lora_dropout = 0.1,\n",
    "    bias = 'none',\n",
    "    task_type = 'CAUSAL_LM'\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('Aditya685/riddles_improved_v2')\n",
    "dataset = dataset['train'].train_test_split(test_size =0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['number', 'messages'],\n",
       "        num_rows: 817\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['number', 'messages'],\n",
       "        num_rows: 91\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'number': 142,\n",
       " 'messages': [\"A word of three letters long, which, spoken after 'how,' describes how a rabbit hops.\",\n",
       "  'The answer to this riddle is the word \"hop.\" When you say \"how a rabbit hops,\" the word that describes the way a rabbit moves in short bursts is \"hop.\" Therefore, the word that completes the sentence and follows \"how\" is \"hop.\"\\n\\nIf you have any questions about the answer or need further clarification, please let me know.',\n",
       "  \"So, 'hop' is the answer and it's how rabbits move? But, how do birds fly? What's the word that comes after 'how' for that?\",\n",
       "  'The word that comes after \"how\" to describe how birds fly is \"fly.\" Birds are able to move through the air by flapping their wings, which allows them to generate lift and stay aloft. The word \"fly\" is the verb that describes this action.\\n\\nSo, to summarize:\\n\\n* \"How a rabbit hops?\" - Answer: \"hop\"\\n* \"How a bird flies?\" - Answer: \"fly\"\\n\\nI hope this helps clarify any confusion. Let me know if you have any other questions!']}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply ChatML format and tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "\n",
    "# Format (ChatML) and tokenize dataset\n",
    "templates = [\n",
    "    '<|im_start|>assistant\\n{msg}<|im_end|>',\n",
    "    '<|im_start|>user\\n{msg}<|im_end|>'\n",
    "]\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "\n",
    "def tokenize(input, max_length):\n",
    "    input_ids, attention_mask , labels = [], [], []\n",
    "    \n",
    "    for i , msg in enumerate(input['messages']):\n",
    "        isHuman = i%2 == 0\n",
    "        msg_chatml = templates[isHuman].format(msg = msg)\n",
    "        msg_tokenized = tokenizer(msg_chatml, truncation=False, add_special_tokens=False)\n",
    "\n",
    "        input_ids+= msg_tokenized['input_ids']\n",
    "        attention_mask+= msg_tokenized['attention_mask']\n",
    "        labels+= [IGNORE_INDEX] * len(msg_tokenized['input_ids']) if isHuman else msg_tokenized['input_ids']\n",
    "\n",
    "    return {\n",
    "        'input_ids' : input_ids[:max_length],\n",
    "        'attention_mask' : attention_mask[:max_length],\n",
    "        'labels': labels[:max_length]\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "dataset_tokenized = dataset.map(\n",
    "    partial(tokenize, max_length = 1024), # max sample length 1024 tokens, enough for this dataset\n",
    "    batched = False,\n",
    "    num_proc = os.cpu_count(), # multithreaded\n",
    "    remove_columns = dataset['train'].column_names # don't need this anymore\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 817\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 91\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longest sample: 878 tokens\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample size distribution\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = [len(tok) for tok in (dataset_tokenized[\"train\"][\"input_ids\"]+dataset_tokenized[\"test\"][\"input_ids\"])] \n",
    "print(f\"longest sample: {max(data)} tokens\")\n",
    "\n",
    "plt.hist(data, bins=10)  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a collate function,train on answers only\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate function - to transform list of dictionaries [ {input_ids: [123, ..]}, {.. ] to single batch dictionary { input_id}\n",
    "\n",
    "def collate(elements):\n",
    "    tokens = [e['input_ids'] for e in elements]\n",
    "    tokens_maxlen = max([len(t) for t  in tokens])\n",
    "\n",
    "    for i, sample in enumerate(elements):\n",
    "        input_ids = sample['input_ids']\n",
    "        labels = sample['labels']\n",
    "        attention_mask = sample['attention_mask']\n",
    "\n",
    "        pad_len = tokens_maxlen - len(input_ids)\n",
    "\n",
    "        input_ids.extend(pad_len * [tokenizer.pad_token_id])\n",
    "        labels.extend(pad_len * [IGNORE_INDEX])\n",
    "        attention_mask.extend(pad_len * [0])\n",
    "\n",
    "    batch = {\n",
    "        'input_ids' : torch.tensor([e['input_ids'] for e in elements]),\n",
    "        'labels' : torch.tensor([e['labels'] for e in elements]),\n",
    "        'attention_mask' : torch.tensor([e['attention_mask'] for e in elements])\n",
    "\n",
    "    }\n",
    "\n",
    "    return batch\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "#### Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# dataset- specific parameters\n",
    "bs = 1 # batch size for training\n",
    "bs_eval = 16 # batch size for evaluation\n",
    "ga_steps = 16 # gradient accumulation steps\n",
    "lr = 0.00002 # learning rate\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "steps_per_epoch = len(dataset_tokenized['train']) // (bs * ga_steps)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"out\",\n",
    "    per_device_train_batch_size=bs,\n",
    "    per_device_eval_batch_size=bs_eval,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    eval_steps=steps_per_epoch//2,    # 2 evals per epoch\n",
    "    save_steps=steps_per_epoch,     # save once per epoch\n",
    "    gradient_accumulation_steps=ga_steps,\n",
    "    num_train_epochs=epochs,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    optim=\"paged_adamw_32bit\",      # val_loss will go nan with paged_adamw_8bit\n",
    "    learning_rate=lr,\n",
    "    group_by_length=False,\n",
    "    bf16=True,        \n",
    "    ddp_find_unused_parameters=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=collate,\n",
    "    train_dataset=dataset_tokenized[\"train\"],\n",
    "    eval_dataset=dataset_tokenized[\"test\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madityaverma685\u001b[0m (\u001b[33maditya685\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/teamspace/studios/this_studio/wandb/run-20240204_093045-k5u0hwmr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aditya685/phi2/runs/k5u0hwmr' target=\"_blank\">testrun</a></strong> to <a href='https://wandb.ai/aditya685/phi2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aditya685/phi2' target=\"_blank\">https://wandb.ai/aditya685/phi2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aditya685/phi2/runs/k5u0hwmr' target=\"_blank\">https://wandb.ai/aditya685/phi2/runs/k5u0hwmr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# wandb project\n",
    "import wandb\n",
    "run = wandb.init(\n",
    "    project = 'phi2',\n",
    "    name = 'testrun'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='1020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   6/1020 00:29 < 2:05:03, 0.14 it/s, Epoch 0.10/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1020, training_loss=0.7154110809167226, metrics={'train_runtime': 8046.2264, 'train_samples_per_second': 2.031, 'train_steps_per_second': 0.127, 'total_flos': 7.931565689017344e+16, 'train_loss': 0.7154110809167226, 'epoch': 19.98})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "# del model\n",
    "\n",
    "gc.collect()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge LoRA adapters with base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30223aa0459847bb8c19cdd880aec8e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# base model\n",
    "base_path = 'microsoft/phi-2'\n",
    "\n",
    "# adapters: path to folder with adapter_model.safetensors\n",
    "adapter_path = 'out/checkpoint-1020'\n",
    "\n",
    "# model dir\n",
    "save_to = 'trained_model'\n",
    "\n",
    "# load model and tokenizer\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_path,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    device_map = 'auto'\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_path)\n",
    "\n",
    "# Add/set tokens same tokens to base model before merging, like we did before training  \n",
    "tokenizer.add_tokens([\"<|im_start|>\", \"<PAD>\"])\n",
    "tokenizer.pad_token = \"<PAD>\"\n",
    "tokenizer.add_special_tokens(dict(eos_token=\"<|im_end|>\"))\n",
    "\n",
    "# Add ChatML template to tokenizer\n",
    "tokenizer.chat_template=\"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n",
    "\n",
    "base_model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Set a default Generation configuration: Llama precise\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=100, \n",
    "    temperature=0.7,\n",
    "    top_p=0.1,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.18,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# Load LoRA and merge\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "model.save_pretrained(save_to, safe_serialization=True, max_shard_size='4GB')\n",
    "tokenizer.save_pretrained(save_to)\n",
    "generation_config.save_pretrained(save_to)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "# del base_model\n",
    "# del model\n",
    "\n",
    "gc.collect()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f9beab7cb9481e83a8c12befb3a6a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am an AI language model designed to assist users with their questions and tasks. I do not have a physical presence or the ability to know personal information about individuals. If there is any specific question related to this topic that you would like me to help answer, please let me know.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "model_path=\"trained_model\"    \n",
    "question=\"Hello, who are you?\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,    \n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path) \n",
    "\n",
    "messages=[\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "        \n",
    "input_tokens = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "output_tokens = model.generate(input_tokens)\n",
    "output = tokenizer.decode(\n",
    "    output_tokens[0][len(input_tokens[0]):],\n",
    "    skip_special_tokens=True\n",
    "    )               \n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(question):\n",
    "    messages=[\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "        \n",
    "    input_tokens = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    output_tokens = model.generate(input_tokens)\n",
    "    output = tokenizer.decode(\n",
    "        output_tokens[0][len(input_tokens[0]):],\n",
    "        skip_special_tokens=True\n",
    "        )               \n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am an artificial intelligence designed to assist and answer questions. I do not have a creator or origin, as I was built using complex algorithms and data from various sources. If there is any specific information you would like me to clarify about my capabilities or functions, please let me know.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response('who created you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I\\'m an AI language model and I don\\'t have the ability to create or access specific resources like \"kaggle.\" However, based on my knowledge of similar concepts, I can help you understand what \"kaggle\" might be referring to.\\n\\nKaggle is a platform that provides data science competitions for researchers, developers, and organizations around the world. It allows participants to compete against each other by submitting their models and predictions to solve various problems using large datasets. The platform also'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response('what is kaggle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b6d181a91824acba6ab893e70ac7e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4930bcda034ddab5755d52a40b30af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d94b99720bd4241a39274430d18ecd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68c2860ac79f4fb7b963b3ce29fb7777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8251c27755d40bf8a8128fc608196b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Aditya685/phi-2_riddles-evolved_epoch20/commit/8e7dc1e90c706ae7a92fc1cae61a018e22c1b597', commit_message='Upload tokenizer', commit_description='', oid='8e7dc1e90c706ae7a92fc1cae61a018e22c1b597', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"Aditya685/phi-2_riddles-evolved_epoch20\")\n",
    "tokenizer.push_to_hub(\"Aditya685/phi-2_riddles-evolved_epoch20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
